{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!find /content/test_data_v2 -maxdepth 3 -type f | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuzR7rX-wODe",
        "outputId": "f2e448b9-600b-4c64-cab4-a6e0e5ccf4b3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/test_data_v2/test_data_v2/050bc88938bd4e739d075269fb2da9b4.jpg\n",
            "/content/test_data_v2/test_data_v2/fea6699b3e5e487da4cb28b68fcd476a.jpg\n",
            "/content/test_data_v2/test_data_v2/1bc52a6203c74518800823a4df70906e.jpg\n",
            "/content/test_data_v2/test_data_v2/f980c9c9d1b6443e95cf43b6ef245dfd.jpg\n",
            "/content/test_data_v2/test_data_v2/c1e29ca8e0b24dd8bdb1f09cd05eaf6e.jpg\n",
            "/content/test_data_v2/test_data_v2/dc05fc8797e9408d86529a58f1a4b1c6.jpg\n",
            "/content/test_data_v2/test_data_v2/c992a6424951495390fd5f90ac9e8672.jpg\n",
            "/content/test_data_v2/test_data_v2/40e5cbd3c4c643c79c80c2fd7eaab876.jpg\n",
            "/content/test_data_v2/test_data_v2/209a710959ab4a0bb4f760550b546ab3.jpg\n",
            "/content/test_data_v2/test_data_v2/2c39356dd6534232b52f6b5a0a4254ac.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n"
      ],
      "metadata": {
        "id": "vT2l3o909RZD"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Stat_362_final_project/train.csv\")\n",
        "test  = pd.read_csv(\"/content/drive/MyDrive/Stat_362_final_project/test.csv\")\n",
        "\n",
        "print(train.columns)\n",
        "print(test.columns)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYuv2D7_pSeP",
        "outputId": "90933829-2a1c-4c7f-b11a-0db6070bf1c8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'file_name', 'label'], dtype='object')\n",
            "Index(['id'], dtype='object')\n",
            "   Unnamed: 0                                        file_name  label\n",
            "0           0  train_data/a6dcb93f596a43249135678dfcfc17ea.jpg      1\n",
            "1           1  train_data/041be3153810433ab146bc97d5af505c.jpg      0\n",
            "2           2  train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg      1\n",
            "3           3  train_data/8542fe161d9147be8e835e50c0de39cd.jpg      0\n",
            "4           4  train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg      1\n",
            "                                                  id\n",
            "0  test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg\n",
            "1  test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg\n",
            "2  test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg\n",
            "3  test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg\n",
            "4  test_data_v2/a16495c578b7494683805484ca27cf9f.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Stat_362_final_project/train.csv\")\n",
        "test  = pd.read_csv(\"/content/drive/MyDrive/Stat_362_final_project/test.csv\")\n",
        "\n",
        "train['filepath'] = train['file_name'].apply(lambda x: \"/content/train_data/\" + x)\n",
        "test['filepath'] = test['id'].apply(\n",
        "    lambda x: \"/content/test_data_v2/\" + x\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train.head(), test.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSmH0OUpW3H",
        "outputId": "d30fab43-afef-491f-c57f-9802621b4ff5"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   Unnamed: 0                                        file_name  label  \\\n",
              " 0           0  train_data/a6dcb93f596a43249135678dfcfc17ea.jpg      1   \n",
              " 1           1  train_data/041be3153810433ab146bc97d5af505c.jpg      0   \n",
              " 2           2  train_data/615df26ce9494e5db2f70e57ce7a3a4f.jpg      1   \n",
              " 3           3  train_data/8542fe161d9147be8e835e50c0de39cd.jpg      0   \n",
              " 4           4  train_data/5d81fa12bc3b4cea8c94a6700a477cf2.jpg      1   \n",
              " \n",
              "                                             filepath  \n",
              " 0  /content/train_data/train_data/a6dcb93f596a432...  \n",
              " 1  /content/train_data/train_data/041be3153810433...  \n",
              " 2  /content/train_data/train_data/615df26ce9494e5...  \n",
              " 3  /content/train_data/train_data/8542fe161d9147b...  \n",
              " 4  /content/train_data/train_data/5d81fa12bc3b4ce...  ,\n",
              "                                                   id  \\\n",
              " 0  test_data_v2/1a2d9fd3e21b4266aea1f66b30aed157.jpg   \n",
              " 1  test_data_v2/ab5df8f441fe4fbf9dc9c6baae699dc7.jpg   \n",
              " 2  test_data_v2/eb364dd2dfe34feda0e52466b7ce7956.jpg   \n",
              " 3  test_data_v2/f76c2580e9644d85a741a42c6f6b39c0.jpg   \n",
              " 4  test_data_v2/a16495c578b7494683805484ca27cf9f.jpg   \n",
              " \n",
              "                                             filepath  \n",
              " 0  /content/test_data_v2/test_data_v2/1a2d9fd3e21...  \n",
              " 1  /content/test_data_v2/test_data_v2/ab5df8f441f...  \n",
              " 2  /content/test_data_v2/test_data_v2/eb364dd2dfe...  \n",
              " 3  /content/test_data_v2/test_data_v2/f76c2580e96...  \n",
              " 4  /content/test_data_v2/test_data_v2/a16495c578b...  )"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    train,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=train['label']\n",
        ")\n"
      ],
      "metadata": {
        "id": "z7vBr8WwqE8W"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "bad_images = []\n",
        "root = \"/content/train_data/train_data\"  # your actual train directory\n",
        "\n",
        "for fname in os.listdir(root):\n",
        "    fpath = os.path.join(root, fname)\n",
        "    try:\n",
        "        img = Image.open(fpath)\n",
        "        img.verify()   # check integrity\n",
        "    except Exception as e:\n",
        "        bad_images.append(fpath)\n",
        "\n",
        "len(bad_images), bad_images[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL3naXbxzqzC",
        "outputId": "9bd520b1-bb10-4904-9dde-998faec27740"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, [])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f in bad_images:\n",
        "    os.remove(f)\n",
        "\n",
        "print(\"Removed\", len(bad_images), \"bad images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K4SIKd-zxio",
        "outputId": "564ee702-68b5-4284-86f7-6fa5a7fe9a61"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 0 bad images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def create_generators(train_df, val_df, test_df):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1\n",
        "    )\n",
        "\n",
        "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        x_col='filepath',\n",
        "        y_col='label',\n",
        "        target_size=(160, 160),\n",
        "        class_mode='raw',\n",
        "        batch_size=16,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = test_val_datagen.flow_from_dataframe(\n",
        "        dataframe=val_df,\n",
        "        x_col='filepath',\n",
        "        y_col='label',\n",
        "        target_size=(160, 160),\n",
        "        class_mode='raw',\n",
        "        batch_size=16,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_generator = test_val_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        x_col='filepath',\n",
        "        y_col=None,\n",
        "        target_size=(160, 160),\n",
        "        class_mode=None,\n",
        "        batch_size=16,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n",
        "\n",
        "train_gen, val_gen, test_gen = create_generators(train_df, val_df, test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj9xT1WZqGWL",
        "outputId": "cdcbaecb-4665-419f-a6b0-17383e12796f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 63959 validated image filenames.\n",
            "Found 15990 validated image filenames.\n",
            "Found 5540 validated image filenames.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/legacy/preprocessing/image.py:920: UserWarning: Found 1 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(*IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model_mnv2 = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_mnv2.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_mnv2.summary()\n",
        "\n",
        "history_mnv2 = model_mnv2.fit(\n",
        "    train_gen,\n",
        "    epochs=5,\n",
        "    validation_data=val_gen\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "WVyWvrpoqIj4",
        "outputId": "3ce7eb8f-4beb-4fff-f5ea-399a1667c722"
      },
      "execution_count": 74,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_160            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_160            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,081</span> (9.24 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,081\u001b[0m (9.24 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,097</span> (641.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,097\u001b[0m (641.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3998/3998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 146ms/step - accuracy: 0.8575 - loss: 0.3265 - val_accuracy: 0.9031 - val_loss: 0.2330\n",
            "Epoch 2/5\n",
            "\u001b[1m3998/3998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 139ms/step - accuracy: 0.9088 - loss: 0.2215 - val_accuracy: 0.9181 - val_loss: 0.2001\n",
            "Epoch 3/5\n",
            "\u001b[1m3998/3998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 137ms/step - accuracy: 0.9190 - loss: 0.1962 - val_accuracy: 0.9197 - val_loss: 0.1930\n",
            "Epoch 4/5\n",
            "\u001b[1m3998/3998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 139ms/step - accuracy: 0.9263 - loss: 0.1814 - val_accuracy: 0.9131 - val_loss: 0.2184\n",
            "Epoch 5/5\n",
            "\u001b[1m3998/3998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 139ms/step - accuracy: 0.9318 - loss: 0.1689 - val_accuracy: 0.9133 - val_loss: 0.2208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Training Accuracy:\", history_mnv2.history[\"accuracy\"][-1])\n",
        "print(\"Final Validation Accuracy:\", history_mnv2.history[\"val_accuracy\"][-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GevotWi4I_No",
        "outputId": "9158403c-36d7-478d-a80b-4cbae2f202f0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Training Accuracy: 0.9306743144989014\n",
            "Final Validation Accuracy: 0.9132583141326904\n"
          ]
        }
      ]
    }
  ]
}